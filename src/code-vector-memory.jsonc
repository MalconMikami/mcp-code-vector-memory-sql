{
  // mcp-code-vector-memory-sql configuration (JSONC)
  // Precedence: environment variables > this file > code defaults
  //
  // Tip: set a value to null to behave as "not set" (use code defaults).
  //
  // This file is read from: src/code-vector-memory.jsonc

  // -----------------------------
  // Storage / workspace
  // -----------------------------

  "CODE_MEMORY_WORKSPACE": ".", // Root for MCP resources (default: current working dir).

  // Required: libSQL URL (local sqld on localhost, or remote Turso).
  // Example local (sqld on localhost): "libsql://127.0.0.1:8080"
  // Example remote (Turso): "libsql://your-db.turso.io"
  "CODE_MEMORY_DB_URL": null,
  "CODE_MEMORY_DB_AUTH_TOKEN": null,

  // -----------------------------
  // Embeddings
  // -----------------------------

  "CODE_MEMORY_EMBED_MODEL": "BAAI/bge-small-en-v1.5", // fastembed model name.
  "CODE_MEMORY_EMBED_DIM": 384, // Required when using a non-default model (must match the model's embedding dimension).
  "CODE_MEMORY_MODEL_DIR": "~/.cache/code-memory", // Cache directory for embedding models and downloaded GGUF.

  // -----------------------------
  // Logging
  // -----------------------------

  "CODE_MEMORY_LOG_LEVEL": "ERROR", // One of: DEBUG, INFO, WARNING, ERROR.
  "CODE_MEMORY_LOG_DIR": null, // Directory for timestamped log files (preferred over LOG_FILE if both set).
  "CODE_MEMORY_LOG_FILE": null, // If set, its parent directory is used for a timestamped log file.

  // -----------------------------
  // Features
  // -----------------------------
  // Vector, FTS and Graph are always enabled in this project.

  // -----------------------------
  // Retrieval / ranking
  // -----------------------------

  "CODE_MEMORY_TOP_K": 12, // Default limit returned by search_memory (when client doesn't pass limit).
  "CODE_MEMORY_TOP_P": 0.6, // Recency filter after re-ranking (0..1], 1.0 keeps all candidates.
  "CODE_MEMORY_OVERSAMPLE_K": 4, // Candidates fetched before re-ranking = limit * oversample_k.
  "CODE_MEMORY_PRIORITY_WEIGHT": 0.15, // Penalty weight for lower priority (priority is 1..5, 1 is most important).
  "CODE_MEMORY_RECENCY_WEIGHT": 0.2, // Recency penalty weight (older items get worse score).
  "CODE_MEMORY_FTS_BONUS": 0.1, // Score bonus (subtracted) for items matched by FTS.

  // -----------------------------
  // Session fallback
  // -----------------------------

  "CODE_MEMORY_SESSION_ID": null, // Fallback session id if the client doesn't send one (most clients should send it).

  // -----------------------------
  // Session-aware ranking (optional)
  // -----------------------------

  // When a session_id is provided to search_memory (argument, ctx.session_id, or CODE_MEMORY_SESSION_ID),
  // results from that same session get a score boost (lower score = ranked higher).
  "CODE_MEMORY_SESSION_BONUS": 0.2,
  "CODE_MEMORY_CROSS_SESSION_PENALTY": 0.0,

  // -----------------------------
  // Local NER (GGUF / llama.cpp)
  // -----------------------------

  // `CODE_MEMORY_NER_MODEL` can be either:
  // - a local .gguf file path (example: "C:/models/qwen.gguf")
  // - a Hugging Face repo id (example: "Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF")
  //
  // Recommended: set to a Hugging Face repo id so the server can auto-download a GGUF when needed.
  // Example URL: https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF
  "CODE_MEMORY_NER_MODEL": "Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF",
  "CODE_MEMORY_NER_AUTO_DOWNLOAD": true, // Set false to disable any Hugging Face downloads.

  "CODE_MEMORY_AUTO_INSTALL": true, // Auto-install llama-cpp-python if missing.
  "CODE_MEMORY_PIP_ARGS": "" // Extra pip args for llama-cpp-python install (advanced).
}
