# Models

`mcp-code-vector-memory-sql` uses two different model types:

- **Embedding model**: turns text into vectors for semantic search (fastembed)
- **Local summary model (optional)**: produces summaries locally from a GGUF file (llama.cpp)

This page explains the tradeoffs and how to switch models safely.

## Embedding models (fastembed)

Embeddings are generated by `fastembed` (CPU) and stored/queried via `sqlite-vec`.

### How to list supported embedding models

`fastembed` supports a fixed list of ONNX models.

```bash
python -c "from fastembed import TextEmbedding; print([m['model'] for m in TextEmbedding.list_supported_models()])"
```

### Recommended options

The best model depends on your tradeoff between speed, memory, and retrieval
quality.

| Model | Why you would pick it |
| --- | --- |
| `BAAI/bge-small-en-v1.5` (default) | Very fast on CPU, small vectors (384-dim), strong baseline for English developer workflows |
| `snowflake/snowflake-arctic-embed-s` | Quality-focused model while still keeping 384-dim vectors; good general semantic retrieval |
| `nomic-ai/nomic-embed-text-v1.5` | Larger 768-dim model that handles long inputs (8192 token truncation) better; useful for longer context chunks |

Notes:

- Some embedding families work best with different prefixes for queries vs documents.
  `mcp-code-vector-memory-sql` currently embeds raw strings (no prefixes). If your model card
  recommends prefixes, consider adding them in your content or customizing the
  embedding pipeline.

### Other supported models

You can find more options by browsing Hugging Face and cross-checking them
against `TextEmbedding.list_supported_models()` (fastembed only supports a
curated set of ONNX models).

### About `Qwen2.5-Embedding-0.6B`

`Qwen2.5-Embedding-0.6B` is a strong embedding model option (especially for
multilingual/code-heavy content), but it is **not** in the `fastembed` supported
list for the current dependencies in this repo.

To use it you would need to:

1. Replace/extend the embedding backend (for example using `transformers`,
   `sentence-transformers`, or an embedding server), and
2. Ensure you set `CODE_MEMORY_EMBED_DIM` and rebuild the DB so sqlite-vec uses
   the correct vector size.

If you want, I can implement an alternate embedding backend behind a feature
flag.

### Switching embedding models safely

Because sqlite-vec tables have a fixed vector size, switching embedding models
usually means switching the DB file too.

Checklist:

1. Pick a model supported by `fastembed` (or change the backend)
2. Set `CODE_MEMORY_EMBED_MODEL`
3. Set the correct `CODE_MEMORY_EMBED_DIM`
4. Use a new DB path (`CODE_MEMORY_DB_PATH`) or delete the old DB

## Local summary models (GGUF)

Local summaries are optional and run via `llama-cpp-python`. Any GGUF model that
works with llama.cpp should work here.

### `Qwen2.5-Coder-0.5B`

`Qwen2.5-Coder-0.5B` is a good default when you want:

- very fast local summaries on CPU
- code-aware text generation
- low RAM usage

Tradeoff: being very small, it can be less accurate/robust than larger models.

### Finding GGUF models

Most people discover GGUF builds on the Hugging Face hub by searching for:

- the model name + `GGUF` (for example `Qwen2.5-Coder-0.5B GGUF`)
- a specific quantization (for example `Q4_K_M`)

Always verify the model license and the publisher you trust.
