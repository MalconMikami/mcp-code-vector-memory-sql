# Models

`mcp-code-vector-memory-sql` uses two different model types:

- **Embedding model**: turns text into vectors for semantic search (fastembed)
- **Local summary model (optional)**: produces summaries locally from a GGUF file (llama.cpp)

This page explains the tradeoffs and how to switch models safely.

## Embedding models (fastembed)

Embeddings are generated by `fastembed` (CPU) and stored/queried via `sqlite-vec`.

### How to list supported embedding models

`fastembed` supports a fixed list of ONNX models.

```bash
python -c "from fastembed import TextEmbedding; print([m['model'] for m in TextEmbedding.list_supported_models()])"
```

### Recommended options

The best model depends on your tradeoff between speed, memory, and retrieval
quality.

| Model | Dimensions | Est. RAM | Max tokens | Why you would pick it |
| --- | ---: | ---: | ---: | --- |
| `BAAI/bge-small-en-v1.5` (default) | 384 | ~100MB | 512 | Pure speed: great cost/benefit for simple CPUs and low-latency developer workflows |
| `snowflake/snowflake-arctic-embed-s` | 384 | ~110MB | 512 | Technical precision: often better for structured/technical retrieval (symbols, function names) when BGE feels "confused" |
| `nomic-ai/nomic-embed-text-v1.5` | 768 | ~280MB | 8192 | Huge window: index longer chunks (long docs / larger files) with less truncation; higher RAM and you must set `CODE_MEMORY_EMBED_DIM=768` |

Notes:

- Some embedding families work best with different prefixes for queries vs documents.
  `mcp-code-vector-memory-sql` currently embeds raw strings (no prefixes). If your model card
  recommends prefixes, consider adding them in your content or customizing the
  embedding pipeline.

### Other supported models

You can find more options by browsing Hugging Face and cross-checking them
against `TextEmbedding.list_supported_models()` (fastembed only supports a
curated set of ONNX models).

### About `Qwen2.5-Embedding-0.6B`

`Qwen2.5-Embedding-0.6B` is a strong embedding model option (especially for
multilingual/code-heavy content), but it is **not** in the `fastembed` supported
list for the current dependencies in this repo.

To use it you would need to:

1. Replace/extend the embedding backend (for example using `transformers`,
   `sentence-transformers`, or an embedding server), and
2. Ensure you set `CODE_MEMORY_EMBED_DIM` and rebuild the DB so sqlite-vec uses
   the correct vector size.

If you want, I can implement an alternate embedding backend behind a feature
flag.

### Switching embedding models safely

Because sqlite-vec tables have a fixed vector size, switching embedding models
usually means switching the DB file too.

Checklist:

1. Pick a model supported by `fastembed` (or change the backend)
2. Set `CODE_MEMORY_EMBED_MODEL`
3. Set the correct `CODE_MEMORY_EMBED_DIM`
4. Use a new DB path (`CODE_MEMORY_DB_PATH`) or delete the old DB

## Local summary models (GGUF)

Local summaries are optional and run via `llama-cpp-python`. Any GGUF model that
works with llama.cpp should work here.

Summary models are **separate** from embedding models:

- Embeddings: `CODE_MEMORY_EMBED_MODEL` / `CODE_MEMORY_EMBED_DIM`
- Local summaries: `CODE_MEMORY_SUMMARY_MODEL`

### Recommended GGUF options (nano + small)

Model sizes and RAM usage vary by quantization, context size, and runtime
settings. The ranges below are meant as practical starting points for local
summaries.

| Model | Size (typical) | Est. RAM | Best for | Why you would pick it |
| --- | ---: | ---: | --- | --- |
| `Qwen2.5-Coder-0.5B-Instruct` | ~350MB (Q4_K_M) / ~450MB (Q8_0) | ~400-480MB | Fast local summaries for code/logs | One of the strongest nano code-oriented models; usually distinguishes errors vs logs well |
| `SmolLM2-360M-Instruct` | ~200MB (Q4_K_M) to ~380MB (FP16) | ~250-300MB | Stability-first summarization | Extremely small and fast; great for summarizing retrieved text (RAG output) when you are tight on RAM |
| `Granite-3.0-2B-Instruct` (mobile quant) | ~480MB (IQ2_XS / IQ3_M) | ~450-500MB+ | Higher quality summaries (if you can afford it) | Stronger technical/corporate language, but risky under a strict ~500MB total budget |

Note: the Granite option can exceed a 500MB budget quickly depending on
`CODE_MEMORY_SUMMARY_CTX`, thread count, and what else is running in the same
process/machine.

### Auto-download

If `CODE_MEMORY_SUMMARY_MODEL` is set to a Hugging Face repo id (for example
`Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF`), the server will download a suitable
`.gguf` from that repo (prefers `Q4_K_M` when available) unless you disable it
with `CODE_MEMORY_SUMMARY_AUTO_DOWNLOAD=0`.

### Nano vs small models

`mcp-code-vector-memory-sql` supports both **nano** and **small** GGUF models for
summarization (and anything larger that fits your machine).

Practical tradeoffs:

- **Nano (~0.5B-1B)**: fastest and lowest RAM; best for short, consistent summaries
- **Small (~1B-7B)**: better accuracy/robustness; slower and uses more RAM

### Tuning (important knobs)

The most useful knobs for summary speed/quality:

- `CODE_MEMORY_SUMMARY_CTX`: context size (higher uses more RAM)
- `CODE_MEMORY_SUMMARY_THREADS`: CPU parallelism
- `CODE_MEMORY_SUMMARY_MAX_TOKENS`: cap summary length
- `CODE_MEMORY_SUMMARY_TEMPERATURE` / `CODE_MEMORY_SUMMARY_TOP_P`: determinism vs creativity
- `CODE_MEMORY_SUMMARY_PROMPT`: keep it strict (1-3 sentences)

Example:

```json
{
  "CODE_MEMORY_SUMMARY_MODEL": "C:/models/qwen2.5-coder-0.5b-instruct.gguf",
  "CODE_MEMORY_SUMMARY_THREADS": "6",
  "CODE_MEMORY_SUMMARY_TEMPERATURE": "0.2",
  "CODE_MEMORY_SUMMARY_MAX_TOKENS": "200"
}
```

### Finding GGUF models

Most people discover GGUF builds on the Hugging Face hub by searching for:

- the model name + `GGUF` (for example `Qwen2.5-Coder-0.5B GGUF`)
- a specific quantization (for example `Q4_K_M`)

Always verify the model license and the publisher you trust.
