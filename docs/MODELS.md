# Models

`mcp-code-vector-memory-sql` uses two different model types:

- **Embedding model**: turns text into vectors for semantic search (fastembed)
- **Local NER model (optional)**: extracts entities/relations locally from a GGUF file (llama.cpp)

This page explains the tradeoffs and how to switch models safely.

## Embedding models (fastembed)

Embeddings are generated by `fastembed` (CPU) and stored/queried via libSQL vector
(when using `CODE_MEMORY_DB_URL`).

### How to list supported embedding models

`fastembed` supports a fixed list of ONNX models.

```bash
python -c "from fastembed import TextEmbedding; print([m['model'] for m in TextEmbedding.list_supported_models()])"
```

### Recommended options

The best model depends on your tradeoff between speed, memory, and retrieval
quality.

| Model | Dimensions | Est. RAM | Max tokens | Why you would pick it |
| --- | ---: | ---: | ---: | --- |
| `BAAI/bge-small-en-v1.5` (default) | 384 | ~100MB | 512 | Pure speed: great cost/benefit for simple CPUs and low-latency developer workflows |
| `snowflake/snowflake-arctic-embed-s` | 384 | ~110MB | 512 | Technical precision: often better for structured/technical retrieval (symbols, function names) when BGE feels "confused" |
| `nomic-ai/nomic-embed-text-v1.5` | 768 | ~280MB | 8192 | Huge window: index longer chunks (long docs / larger files) with less truncation; higher RAM and you must set `CODE_MEMORY_EMBED_DIM=768` |

Notes:

- Some embedding families work best with different prefixes for queries vs documents.
  `mcp-code-vector-memory-sql` currently embeds raw strings (no prefixes). If your model card
  recommends prefixes, consider adding them in your content or customizing the
  embedding pipeline.

### Other supported models

You can find more options by browsing Hugging Face and cross-checking them
against `TextEmbedding.list_supported_models()` (fastembed only supports a
curated set of ONNX models).

### About `Qwen2.5-Embedding-0.6B`

`Qwen2.5-Embedding-0.6B` is a strong embedding model option (especially for
multilingual/code-heavy content), but it is **not** in the `fastembed` supported
list for the current dependencies in this repo.

To use it you would need to:

1. Replace/extend the embedding backend (for example using `transformers`,
   `sentence-transformers`, or an embedding server), and
2. Ensure you set `CODE_MEMORY_EMBED_DIM` to match your embedding model.
   the correct vector size.

If you want, I can implement an alternate embedding backend behind a feature
flag.

### Switching embedding models safely

Because the embedding dimension is fixed in the schema (libSQL `F32_BLOB(dim)`),
switching embedding models
usually means switching the DB file too.

Checklist:

1. Pick a model supported by `fastembed` (or change the backend)
2. Set `CODE_MEMORY_EMBED_MODEL`
3. Set the correct `CODE_MEMORY_EMBED_DIM`
4. Use a new libSQL database (new URL/db) or clear the existing one

## Local NER models (GGUF)

Local entity/relation extraction (NER) is optional and runs via `llama-cpp-python`.
Any GGUF model that works with llama.cpp should work here.

NER models are **separate** from embedding models:

- Embeddings: `CODE_MEMORY_EMBED_MODEL` / `CODE_MEMORY_EMBED_DIM`
- Local NER: `CODE_MEMORY_NER_MODEL`

### Recommended GGUF options (nano + small)

Model sizes and RAM usage vary by quantization, context size, and runtime
settings. The ranges below are meant as practical starting points for local
NER.

| Model | Size (typical) | Est. RAM | Best for | Why you would pick it |
| --- | ---: | ---: | --- | --- |
| `Qwen2.5-Coder-0.5B-Instruct` | ~350MB (Q4_K_M) / ~450MB (Q8_0) | ~400-480MB | Fast local NER for software text | One of the strongest nano code-oriented models; usually extracts errors/tech/services well |
| `SmolLM2-360M-Instruct` | ~200MB (Q4_K_M) to ~380MB (FP16) | ~250-300MB | Stability-first extraction | Extremely small and fast; good when you are tight on RAM |
| `Granite-3.0-2B-Instruct` (mobile quant) | ~480MB (IQ2_XS / IQ3_M) | ~450-500MB+ | Higher quality extraction (if you can afford it) | Stronger language robustness, but risky under a strict ~500MB total budget |

Note: the Granite option can exceed a 500MB budget quickly depending on
`CODE_MEMORY_NER_CTX`, thread count, and what else is running in the same
process/machine.

### Auto-download

If `CODE_MEMORY_NER_MODEL` is set to a Hugging Face repo id (for example
`Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF`), the server will download a suitable
`.gguf` from that repo (prefers `Q4_K_M` when available) unless you disable it
with `CODE_MEMORY_NER_AUTO_DOWNLOAD=0`.

### Nano vs small models

`mcp-code-vector-memory-sql` supports both **nano** and **small** GGUF models for
NER extraction (and anything larger that fits your machine).

Practical tradeoffs:

- **Nano (~0.5B-1B)**: fastest and lowest RAM; best for short, consistent summaries
- **Small (~1B-7B)**: better accuracy/robustness; slower and uses more RAM

### Tuning (important knobs)

The most useful knobs for NER speed/quality:

- `CODE_MEMORY_NER_CTX`: context size (higher uses more RAM)
- `CODE_MEMORY_NER_THREADS`: CPU parallelism
- `CODE_MEMORY_NER_MAX_TOKENS`: cap output length
- `CODE_MEMORY_NER_TEMPERATURE` / `CODE_MEMORY_NER_TOP_P`: determinism vs creativity
- `CODE_MEMORY_NER_PROMPT`: keep it strict (JSON only)

Example:

```json
{
  "CODE_MEMORY_NER_MODEL": "C:/models/qwen2.5-coder-0.5b-instruct.gguf",
  "CODE_MEMORY_NER_THREADS": "6",
  "CODE_MEMORY_NER_TEMPERATURE": "0.1",
  "CODE_MEMORY_NER_MAX_TOKENS": "800"
}
```

### Finding GGUF models

Most people discover GGUF builds on the Hugging Face hub by searching for:

- the model name + `GGUF` (for example `Qwen2.5-Coder-0.5B GGUF`)
- a specific quantization (for example `Q4_K_M`)

Always verify the model license and the publisher you trust.
